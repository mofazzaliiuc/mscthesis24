{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test the paramters of the CNN multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D, Conv1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import seed\n",
    "import os\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "method to create lagged features\n",
    "\n",
    "data - data\n",
    "to_keep - number of lagged_features\n",
    "to_remove - number of days to remove\n",
    "\n",
    "\"\"\"\n",
    "def create_lagged_features(data, to_keep=1, to_remove=1):\n",
    "    variables = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    columns, names = list(), list()\n",
    "    \n",
    "    for i in range(to_keep, 0, -1):\n",
    "        columns.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(variables)]\n",
    "\n",
    "    for i in range(0, to_remove):\n",
    "        columns.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(variables)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(variables)]\n",
    "            \n",
    "    #put it all together\n",
    "    final = concat(columns, axis=1)\n",
    "    final.columns = names\n",
    "    \n",
    "    #drop rows with NaN values\n",
    "    final.dropna(inplace=True)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to calculate rsi\n",
    "\n",
    "data - data\n",
    "period - RSI period\n",
    "\n",
    "\"\"\"\n",
    "def rsi(data, period: int = 14):\n",
    "    \n",
    "    delta = data[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = gain / loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "method to load lag dataset\n",
    "\n",
    "step - shows the current iteration - used to know how to shuffle\n",
    "lag_features - the number of lagged features\n",
    "train_ratio - percentage of dataset to use as training data\n",
    "lag_granularity - day or hours - the type of lag for dataset to import\n",
    "lag - day or hours - actual value of lag of dataset to import\n",
    "dataset_grouped_by - shows whether dataset is grouped daily or hourly\n",
    "cleaned - whether or not to use cleaned data\n",
    "\n",
    "\"\"\"\n",
    "def load_lag_sets(step, lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned):\n",
    "    analyser = \"vader\"\n",
    "    \n",
    "    #read dataset\n",
    "    folder = \"./../../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets\"\n",
    "    #add cleaned if to use cleaned data\n",
    "    if cleaned:\n",
    "        folder = folder + '/cleaned'\n",
    "    #get full filename\n",
    "    filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    #group by datetime\n",
    "    df = df.groupby('DateTime').agg(lambda x: x.mean())\n",
    "    \n",
    "    #calculate change\n",
    "    df[\"Change\"] = (df[\"Close\"] - df[\"Close\"].shift(1)).astype(float)\n",
    "    #drop empty\n",
    "    df = df.dropna(subset=['Change'])\n",
    "    #max positive change \n",
    "    max_change = df[\"Change\"].max()\n",
    "    #max negative change \n",
    "    min_change = df[\"Change\"].min()\n",
    "    \n",
    "    #prepare bins\n",
    "    rnge = max_change - min_change\n",
    "    bin_size = (max_change - min_change) / 10\n",
    "    half_range = rnge/2\n",
    "    bins = np.arange(-1*half_range, half_range, bin_size)\n",
    "    bins[5] = 0\n",
    "    bins[0] = float(\"-inf\")\n",
    "    bins = np.append(bins, float(\"inf\"))\n",
    "    #more specific bins\n",
    "    bins = [float(\"-inf\"), -1320, -990, -660, -330, 0., 330, 660, 990, 1320, float(\"inf\")]\n",
    "    labels = [0, 1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    #set bins\n",
    "    df['Change'] = pd.cut(x=df['Change'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    #whether to add RSI, long and short moving averages\n",
    "    add_RSI = False\n",
    "    add_longMAvg = False\n",
    "    add_shortMAvg = False\n",
    "\n",
    "    if(add_RSI):\n",
    "        #calculate RSI\n",
    "        RSI = 14\n",
    "        df['RSI'] = rsi(df, RSI)\n",
    "        df = df.iloc[RSI:]\n",
    "\n",
    "    #calcualte moving averages\n",
    "    if(add_shortMAvg):\n",
    "        short_window = 9\n",
    "        df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        long_window = 21\n",
    "        df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        df = df.iloc[long_window:]\n",
    "    elif(add_RSI):\n",
    "        df = df.iloc[RSI:]\n",
    "    elif(add_shortMAvg):\n",
    "        df = df.iloc[short_window:]\n",
    "        \n",
    "    #keep only wanted columns\n",
    "    features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "    if(add_RSI):\n",
    "        features.append(\"RSI\")\n",
    "\n",
    "    if(add_longMAvg):\n",
    "        features.append(\"long_mavg\")\n",
    "\n",
    "    if(add_shortMAvg):\n",
    "        features.append(\"short_mavg\")\n",
    "\n",
    "    #keep only wanted features\n",
    "    df = df[features]\n",
    "\n",
    "    #creating copy so that data is not loaded once again\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "   #number of previous records to consider for every example\n",
    "    n_lag = lag_features\n",
    "    #number of features\n",
    "    n_features = len(features)\n",
    "    #calculate total_features\n",
    "    total_features = n_lag*n_features\n",
    "\n",
    "    if(total_features == 0):\n",
    "        total_features = n_features\n",
    "        \n",
    "    #add lagged features\n",
    "    df_with_lagged = create_lagged_features(df, n_lag, 1)\n",
    "    df_with_lagged = df_with_lagged.reset_index()\n",
    "    df_with_lagged = df_with_lagged.drop(['DateTime'], axis=1)\n",
    "    \n",
    "    #shuffle\n",
    "    np.random.seed(1)\n",
    "    for j in range(0, step+1):\n",
    "        df_with_lagged = shuffle(df_with_lagged)\n",
    "    \n",
    "    #divide df into train and test\n",
    "    data_len = len(df_with_lagged)\n",
    "    train_size = int(data_len*train_ratio)\n",
    "\n",
    "    #get training data\n",
    "    train = df_with_lagged.iloc[:train_size]\n",
    "    #get testing data\n",
    "    test = df_with_lagged.iloc[train_size:]\n",
    "    \n",
    "    #get labels\n",
    "    train_y = train[\"var1(t)\"].values\n",
    "    test_y = test[\"var1(t)\"].values\n",
    "    \n",
    "    #normalise features\n",
    "    xscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train = xscaler.fit_transform(train)\n",
    "    test = xscaler.transform(test)\n",
    "\n",
    "    train_labels = train_y\n",
    "    test_labels = test_y\n",
    "\n",
    "    #remove the last set of values(data of time to be predicted)\n",
    "    train = train[:, :total_features]\n",
    "    test = test[:, :total_features]\n",
    "\n",
    "    #keep only prices array\n",
    "    train_X, train_y = train[:, :total_features], train_y\n",
    "    test_X, test_y = test[:, :total_features], test_y\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_lag, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "\n",
    "    #get labels as categorical\n",
    "    train_y = keras.utils.to_categorical(train_y, 10)\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y, len(features), df, train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to create a model and test it\n",
    "\n",
    "epochs - number of epochs\n",
    "neurons - number of neurons per layer\n",
    "batch_size - batch size\n",
    "layers - number of layers\n",
    "train_X - training data\n",
    "test_X - testing data\n",
    "train_y - training labels\n",
    "test_y - testing labels\n",
    "lag_features - number of lagged features\n",
    "features - list of features\n",
    "df - whole data\n",
    "train_size - percentage of dataset to use as training data\n",
    "\n",
    "\"\"\"\n",
    "def create_model_test(epochs, neurons, batch_size, layers, train_X, test_X, train_y, test_y, lag_features, features, df, train_size):\n",
    "    \n",
    "    #set seed to reproduce results\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    dropout = 0.25\n",
    "    activ_func = \"linear\"\n",
    "    \n",
    "    #return sequences flag if there are more than 1 layer\n",
    "    return_seq = layers > 1\n",
    "\n",
    "    #add first layer\n",
    "    model.add(Conv1D(neurons, kernel_size=2, padding='same', input_shape=(train_X.shape[1], train_X.shape[2]), activation=activ_func))\n",
    "    model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    \n",
    "    #add the other layers\n",
    "    for i in range(1, layers):\n",
    "        model.add(Conv1D(neurons, kernel_size=2, padding='same', activation=activ_func))\n",
    "        model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        \n",
    "    #flatten and add a dense layer and to output the prediction\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience =20)\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False,validation_split=0.2, callbacks=[callback])\n",
    "        \n",
    "    #reshape\n",
    "    test_X = test_X.reshape((test_X.shape[0], lag_features, features))\n",
    "\n",
    "    #make prediction\n",
    "    pred = model.predict(test_X)\n",
    "\n",
    "    #reshape again\n",
    "    test_X = test_X.reshape((test_X.shape[0], lag_features* features,))\n",
    "        \n",
    "    #get prediction\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    report = sklearn.metrics.classification_report(test_y, pred, zero_division=0,output_dict=True)\n",
    "    \n",
    "    #obtain f1-scores for classes\n",
    "    f1 = np.zeros(10)\n",
    "    for i in range(0,10):\n",
    "        f1[i] = report[str(i)]['f1-score'] if str(i) in report else 0\n",
    "        \n",
    "    preds = len(pred)\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(0, preds):\n",
    "        if((test_y[i] > 4 and pred[i] > 4) or (test_y[i] < 5 and pred[i] < 5)):\n",
    "            correct += 1\n",
    "\n",
    "    acc = (correct/preds)*100\n",
    "        \n",
    "    #return accuracy and f1\n",
    "    return report['accuracy'], f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to implement the running\n",
    "\n",
    "lag_granularity - this can be days or hours - the lag granularity for the dataset to use\n",
    "lag - actual value of lag of dataset to use\n",
    "dataset_grouped_by - whether the dataset to use is grouped daily or hourly - can be day or hour\n",
    "cleaned - whether to use cleaned dataset\n",
    "\n",
    "\"\"\"\n",
    "def implement(lag_granularity, lag, dataset_grouped_by, cleaned):\n",
    "    #build result filename\n",
    "    filename = '../results/cnn_multiclass/cnn__multiclass_groupedby_'+dataset_grouped_by+\"_lag_\"+lag_granularity+\"_\"+str(lag)\n",
    "    if cleaned:\n",
    "        filename = filename + '_cleaned'\n",
    "    full_filename = filename+\".csv\"\n",
    "\n",
    "    columns = [\"lag\", \"batch_size\", \"neurons\", \"layers\", \"split\", \"f1-0\", \"f1-1\", \"f1-2\", \"f1-3\", \"f1-4\", \"f1-5\", \"f1-6\", \"f1-7\", \"f1-8\", \"f1-9\"]\n",
    "    \n",
    "    #try to read data if the result file exists, otherwise create a new dataframe\n",
    "    try:\n",
    "        results = pd.read_csv(full_filename)\n",
    "    except:\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "        \n",
    "    #lagged_features\n",
    "    lags = [1, 3, 7, 14]\n",
    "    #train_ratio\n",
    "    train_ratio = 0.85\n",
    "    \n",
    "    #for each lag feature\n",
    "    for lag_features in lags:\n",
    "        \n",
    "        #combinations\n",
    "        neurons = [16, 32, 64, 128, 256]\n",
    "        layers = [1, 2, 3]\n",
    "        batch_sizes = [5, 20, 50, 80]\n",
    "        \n",
    "        for n in neurons:\n",
    "            for l in layers:\n",
    "                for b in  batch_sizes:\n",
    "                    accuracies = []\n",
    "                    dir_accuracies = []\n",
    "                    f1_0 = []\n",
    "                    f1_1 = []\n",
    "                    f1_2 = []\n",
    "                    f1_3 = []\n",
    "                    f1_4 = []\n",
    "                    f1_5 = []\n",
    "                    f1_6 = []\n",
    "                    f1_7 = []\n",
    "                    f1_8 = []\n",
    "                    f1_9 = []\n",
    "                    print(\"Testing model: lag:\", lag_features, \", neurons:\", n, \", layers:\", l, \", batch_size:\", b)\n",
    "\n",
    "                    for i in range (0,5):\n",
    "                        train_X, test_X, train_y, test_y, features, df, train_size = load_lag_sets(i, lag_features, train_ratio, lag_granularity, lag, dataset_grouped_by, cleaned)\n",
    "                        acc, f1, dir_acc = create_model_test(10000, n, b, l, train_X, test_X, train_y, test_y, lag_features, features, df, train_size)\n",
    "                        accuracies.append(acc)\n",
    "                        dir_accuracies.append(dir_acc)\n",
    "\n",
    "                        f1_0.append(f1[0])\n",
    "                        f1_1.append(f1[1])\n",
    "                        f1_2.append(f1[2])\n",
    "                        f1_3.append(f1[3])\n",
    "                        f1_4.append(f1[4])\n",
    "                        f1_5.append(f1[5])\n",
    "                        f1_6.append(f1[6])\n",
    "                        f1_7.append(f1[7])\n",
    "                        f1_8.append(f1[8])\n",
    "                        f1_9.append(f1[9])\n",
    "\n",
    "\n",
    "                    accuracies = np.array(accuracies)\n",
    "                    dir_accuracies = np.array(dir_accuracies)\n",
    "                    f1_0 = np.array(f1_0)\n",
    "                    f1_1 = np.array(f1_1)\n",
    "                    f1_2 = np.array(f1_2)\n",
    "                    f1_3 = np.array(f1_3)\n",
    "                    f1_4 = np.array(f1_4)\n",
    "                    f1_5 = np.array(f1_5)\n",
    "                    f1_6 = np.array(f1_6)\n",
    "                    f1_7 = np.array(f1_7)\n",
    "                    f1_8 = np.array(f1_8)\n",
    "                    f1_9 = np.array(f1_9)\n",
    "\n",
    "                    mean_acc =accuracies.mean()\n",
    "                    min_acc =accuracies.min()\n",
    "                    max_acc =accuracies.max()\n",
    "                    dir_mean_acc = dir_accuracies.mean()\n",
    "                    dir_max_acc = dir_accuracies.max()\n",
    "                    diff_acc = max_acc - min_acc\n",
    "\n",
    "                    mean_f1_0 = f1_0.mean();\n",
    "                    mean_f1_1 = f1_1.mean();\n",
    "                    mean_f1_2 = f1_2.mean();\n",
    "                    mean_f1_3 = f1_3.mean();\n",
    "                    mean_f1_4 = f1_4.mean();\n",
    "                    mean_f1_5 = f1_5.mean();\n",
    "                    mean_f1_6 = f1_6.mean();\n",
    "                    mean_f1_7 = f1_7.mean();\n",
    "                    mean_f1_8 = f1_8.mean();\n",
    "                    mean_f1_9 = f1_9.mean();\n",
    "\n",
    "                    results = results.append({\"lag\": lag_features, \"batch_size\": b, \"neurons\":n, \"layers\":l, \"split\": train_ratio, \"dir_mean_acc\": dir_mean_acc, \"dir_max_acc\": dir_max_acc, \"mean_acc\": mean_acc, \"min_acc\": min_acc, \"max_acc\": max_acc, \"diff_acc\":diff_acc, \"f1-0\": mean_f1_0, \"f1-1\": mean_f1_1, \"f1-2\": mean_f1_2, \"f1-3\": mean_f1_3, \"f1-4\": mean_f1_4, \"f1-5\": mean_f1_5, \"f1-6\": mean_f1_6, \"f1-7\": mean_f1_7, \"f1-8\": mean_f1_8, \"f1-9\": mean_f1_9}, ignore_index=True)\n",
    "                \n",
    "    return results, full_filename\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag granularity - days or hours\n",
    "lag_granularity = \"days\"\n",
    "#lag value\n",
    "lag = 7\n",
    "#dataset grouped type - day or hour\n",
    "dataset_grouped_by = \"day\"\n",
    "#cleaned\n",
    "cleaned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: lag: 1 , neurons: 16 , layers: 1 , batch_size: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-dd973dead030>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimplement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlag_granularity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_grouped_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-4cdc4aadc6fc>\u001b[0m in \u001b[0;36mimplement\u001b[1;34m(lag_granularity, lag, dataset_grouped_by, cleaned)\u001b[0m\n\u001b[0;32m     55\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                         \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_lag_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag_granularity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_grouped_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                         \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlag_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                         \u001b[0mdir_accuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-3b3726b46047>\u001b[0m in \u001b[0;36mcreate_model_test\u001b[1;34m(epochs, neurons, batch_size, layers, train_X, test_X, train_y, test_y, lag_features, features, df, train_size)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m#reshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results, full_filename = implement(lag_granularity, lag, dataset_grouped_by, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-704b76413c24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create folder if it does not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_filename' is not defined"
     ]
    }
   ],
   "source": [
    "#create folder if it does not exist\n",
    "folder = full_filename.rsplit('/', 1)[0]\n",
    "if not os.path.exists(full_filename):\n",
    "    os.makedirs(full_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(full_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
