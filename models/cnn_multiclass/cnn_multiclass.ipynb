{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for predicting magnitude of price change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "import joblib \n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D, Conv1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import seed\n",
    "import os\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "method to create lagged features\n",
    "\n",
    "data - data\n",
    "to_keep - number of lagged_features\n",
    "to_remove - number of days to remove\n",
    "\n",
    "\"\"\"\n",
    "def create_lagged_features(data, to_keep=1, to_remove=1):\n",
    "    variables = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    columns, names = list(), list()\n",
    "    \n",
    "    for i in range(to_keep, 0, -1):\n",
    "        columns.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(variables)]\n",
    "\n",
    "    for i in range(0, to_remove):\n",
    "        columns.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(variables)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(variables)]\n",
    "            \n",
    "    #put it all together\n",
    "    final = concat(columns, axis=1)\n",
    "    final.columns = names\n",
    "    \n",
    "    #drop rows with NaN values\n",
    "    final.dropna(inplace=True)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to calculate rsi\n",
    "\n",
    "data - data\n",
    "period - RSI period\n",
    "\n",
    "\"\"\"\n",
    "def rsi(data, period: int = 14):\n",
    "    \n",
    "    delta = data[\"Close\"].diff()\n",
    "\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "    up[up < 0] = 0\n",
    "    down[down > 0] = 0\n",
    "\n",
    "    gain = up.ewm(com=(period - 1), min_periods=period).mean()\n",
    "    loss = down.abs().ewm(com=(period - 1), min_periods=period).mean()\n",
    "\n",
    "    RS = gain / loss\n",
    "    return 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lag granularity - days or hours\n",
    "lag_granularity = \"days\"\n",
    "#lag value\n",
    "lag = 3\n",
    "# type of analyser - TextBlob or vader\n",
    "analyser = \"vader\"\n",
    "# analyser = \"TextBlob\"\n",
    "#dataset grouped type - day or hour\n",
    "dataset_grouped_by = \"day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "folder = \"./../../datasets/tweets_prices_volumes_sentiment/\"+analyser+\"/\"+dataset_grouped_by+\"_datasets/cleaned\"\n",
    "filename = folder+\"/final_data_lag_\"+lag_granularity+\"_\"+str(lag)+\".csv\" if (lag > 0) else folder+\"/final_data_no_lag.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by datetime\n",
    "df = df.groupby('DateTime').agg(lambda x: x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate change\n",
    "df[\"Change\"] = (df[\"Close\"] - df[\"Close\"].shift(1)).astype(float)\n",
    "#drop empty\n",
    "df = df.dropna(subset=['Change'])\n",
    "#max positive change \n",
    "max_change = df[\"Change\"].max()\n",
    "#max negative change \n",
    "min_change = df[\"Change\"].min()\n",
    "\n",
    "#prepare bins\n",
    "rnge = max_change - min_change\n",
    "bin_size = (max_change - min_change) / 10\n",
    "half_range = rnge/2\n",
    "bins = np.arange(-1*half_range, half_range, bin_size)\n",
    "bins[5] = 0\n",
    "bins[0] = float(\"-inf\")\n",
    "bins = np.append(bins, float(\"inf\"))\n",
    "#more specific bins\n",
    "bins = [float(\"-inf\"), -1320, -990, -660, -330, 0., 330, 660, 990, 1320, float(\"inf\")]\n",
    "labels = [0, 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#set bins\n",
    "df['Change'] = pd.cut(x=df['Change'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "add_RSI = False\n",
    "add_longMAvg = False\n",
    "add_shortMAvg = False\n",
    "\n",
    "if(add_RSI):\n",
    "    #calcualte RSI\n",
    "    RSI = 14\n",
    "    df['RSI'] = rsi(df, RSI)\n",
    "    df = df.iloc[RSI:]\n",
    "\n",
    "#calculate moving averages\n",
    "if(add_shortMAvg):\n",
    "    short_window = 9\n",
    "    df['short_mavg'] = df.rolling(window=short_window)[\"Close\"].mean()\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    long_window = 21\n",
    "    df[\"long_mavg\"] = df.rolling(window=long_window)[\"Close\"].mean()\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    df = df.iloc[long_window:]\n",
    "elif(add_RSI):\n",
    "    df = df.iloc[RSI:]\n",
    "elif(add_shortMAvg):\n",
    "    df = df.iloc[short_window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only wanted columns\n",
    "features = ['Change', 'subjectivity', 'polarity','Tweet_vol','Volume_(BTC)'] if analyser == \"Textblob\" else ['Change', 'Close', 'pos_pol', 'neg_pol', 'Tweet_vol']\n",
    "\n",
    "if(add_RSI):\n",
    "    features.append(\"RSI\")\n",
    "    \n",
    "if(add_longMAvg):\n",
    "    features.append(\"long_mavg\")\n",
    "    \n",
    "if(add_shortMAvg):\n",
    "    features.append(\"short_mavg\")\n",
    "\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correlation matrix\n",
    "sn.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating copy so that data is not loaded once again\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of previous records to consider for every example\n",
    "n_lag = 3\n",
    "#number of features\n",
    "n_features = len(features)\n",
    "#calculate total_features\n",
    "total_features = n_lag*n_features\n",
    "\n",
    "if(total_features == 0):\n",
    "    total_features = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lagged data to records\n",
    "data_with_lagged = create_lagged_features(df_copy, n_lag, 1)\n",
    "data_with_lagged = data_with_lagged.reset_index()\n",
    "data_with_lagged = data_with_lagged.drop(['DateTime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "np.random.seed(1)\n",
    "#shuffle times \n",
    "shuffle_times = 1;\n",
    "for j in range(0, shuffle_times+1):\n",
    "    data_with_lagged = shuffle(data_with_lagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide df into train and test\n",
    "train_ratio = 0.85\n",
    "data_len = len(data_with_lagged)\n",
    "train_size = int(data_len*train_ratio)\n",
    "\n",
    "train = data_with_lagged.iloc[:train_size]\n",
    "test = data_with_lagged.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare labels\n",
    "train_y = train[\"var1(t)\"].values\n",
    "test_y = test[\"var1(t)\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise features\n",
    "xscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train = xscaler.fit_transform(train)\n",
    "test = xscaler.transform(test)\n",
    "joblib.dump(xscaler, 'saved/scaler.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "train_labels = train_y\n",
    "test_labels = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the last set of values(data of time to be predicted)\n",
    "train = train[:, :total_features]\n",
    "test = test[:, :total_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only prices array\n",
    "train_X, train_y = train[:, :total_features], train_y\n",
    "test_X, test_y = test[:, :total_features], test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_lag, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set labels for training data to categorical\n",
    "train_y = keras.utils.to_categorical(train_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed to reproduce results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "neurons = 128\n",
    "epochs = 10000\n",
    "dropout = 0.25\n",
    "batch_size = 80\n",
    "activ_func = \"linear\"\n",
    "\n",
    "model.add(Conv1D(neurons, kernel_size=2, padding='same', input_shape=(train_X.shape[1], train_X.shape[2]), activation=activ_func))\n",
    "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Conv1D(neurons, kernel_size=2, padding='same', activation=activ_func))\n",
    "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "#flatten and add a dense layer and to output the prediction\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model.png'\n",
    "tf.keras.utils.plot_model(model, to_file=model_file, show_shapes=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience =20)\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=10000, batch_size=batch_size, verbose=2, shuffle=False,validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"saved/ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss graph\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Loss graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training and validation accuracy\n",
    "loss_train = history.history['accuracy']\n",
    "loss_val = history.history['val_accuracy']\n",
    "epochs = range(1,len(loss_val) + 1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape\n",
    "test_X = test_X.reshape((test_X.shape[0], n_lag, n_features))\n",
    "\n",
    "#predict values for test data\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "#reshape again\n",
    "test_X = test_X.reshape((test_X.shape[0], n_lag* n_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change back from categorical\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(test_y, pred,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = len(pred)\n",
    "correct = 0\n",
    "\n",
    "for i in range(0, preds):\n",
    "    if((test_y[i] > 4 and pred[i] > 4) or (test_y[i] < 5 and pred[i] < 5)):\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Direction Accuracy:\", (correct/preds)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
