{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook removes duplicates and non-English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import threading\n",
    "import langdetect as ld\n",
    "from nltk.corpus import words\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"../datasets/general/BTCTWEETS.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "tweet_data = tweet_data.drop_duplicates(subset=['text', 'user']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter =0 \n",
    "tweets = len(tweet_data)\n",
    "def is_english(tweet):\n",
    "    global counter;\n",
    "    counter = counter + 1;\n",
    "    \n",
    "    if(counter % 10000 == 0):\n",
    "        print(\"Completed: \"+ str(counter) +\"/\"+ str(tweets))\n",
    "    try:\n",
    "        return ld.detect(tweet)=='en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non english tweets\n",
    "tweet_data = tweet_data[tweet_data['text'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.to_csv(\"./../datasets/general/BTCTWEETS_english_no_duplicates.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fyp] *",
   "language": "python",
   "name": "conda-env-fyp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
